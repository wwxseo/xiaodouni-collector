import os
import re
import requests
from bs4 import BeautifulSoup

def get_xiaodouni_images():
    """æ›´ç¨³å¥çš„å›¾ç‰‡æŠ“å–é€»è¾‘"""
    print("ğŸš€ å¼€å§‹æœå¯»å°è±†æ³¥...")
    
    # å°è¯•ä¸¤ä¸ªæœç´¢æºï¼Œç¬¬ä¸€ä¸ªæ˜¯é«˜æ¸…è¿‡æ»¤ï¼Œç¬¬äºŒä¸ªæ˜¯æ™®é€šæœç´¢
    search_urls = [
        "https://www.bing.com/images/search?q=å°è±†æ³¥+wallpaper&qft=+filterui:imagesize-large&form=IRFLTR",
        "https://www.bing.com/images/search?q=å°è±†æ³¥"
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    
    images = []
    
    for url in search_urls:
        if len(images) >= 12: break
        try:
            print(f"ğŸ” å°è¯•ä»æºæŠ“å–: {url}")
            response = requests.get(url, headers=headers, timeout=15)
            if response.status_code != 200:
                print(f"âš ï¸ è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                continue
                
            soup = BeautifulSoup(response.text, 'html.parser')
            # å°è¯•è§£æ Bing çš„å›¾ç‰‡å…ƒæ•°æ® m å±æ€§
            tags = soup.find_all("a", class_="iusc")
            print(f"æ‰¾åˆ°å€™é€‰æ ‡ç­¾æ•°: {len(tags)}")
            
            for img_tag in tags:
                m_content = img_tag.get("m")
                if m_content:
                    murl_match = re.search(r'"murl":"(.*?)"', m_content)
                    if murl_match:
                        img_url = murl_match.group(1)
                        # ç®€å•è¿‡æ»¤ä¸€äº›æ— æ•ˆé“¾æ¥
                        if img_url.startswith("http") and not any(x in img_url for x in ["example.com", "thumbnail"]):
                            if img_url not in images:
                                images.append(img_url)
                if len(images) >= 12: break
        except Exception as e:
            print(f"âŒ å½“å‰æºæŠ“å–å¼‚å¸¸: {e}")
            
    print(f"ğŸ¯ æœ€ç»ˆæ•è·å°è±†æ³¥æ•°é‡: {len(images)}")
    return images

def update_readme(urls):
    """æ›´æ–° README.md ä¸­çš„å›¾ç‰‡å¢™"""
    if not urls:
        print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡ï¼Œè·³è¿‡ README æ›´æ–°ã€‚")
        return

    if not os.path.exists("README.md"):
        print("âŒ é”™è¯¯ï¼šREADME.md æ–‡ä»¶ä¸å­˜åœ¨")
        return

    with open("README.md", "r", encoding="utf-8") as f:
        content = f.read()

    # æ„å»º HTML å›¾ç‰‡å¢™
    img_html = '<div align="center">\n'
    for url in urls:
        img_html += f'  <img src="{url}" width="180" alt="å°è±†æ³¥" style="margin:5px; border-radius:12px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">'
    img_html += '\n  <p><i>ğŸ”„ æ¯æ—¥è‡ªåŠ¨æ›´æ–°ï¼Œæœé›†è‡ªå…¨ç½‘å›¾æº</i></p>\n</div>'
    
    # ä¸¥æ ¼åŒ¹é… README ä¸­çš„æ ‡è®°
    pattern = r"<!-- START_SECTION:xiaodouni -->.*?<!-- END_SECTION:xiaodouni -->"
    if not re.search(pattern, content, flags=re.DOTALL):
        print("âŒ é”™è¯¯ï¼šåœ¨ README.md ä¸­æ²¡æ‰¾åˆ° <!-- START_SECTION:xiaodouni --> æ ‡è®°")
        return

    replacement = f"<!-- START_SECTION:xiaodouni -->\n{img_html}\n<!-- END_SECTION:xiaodouni -->"
    new_content = re.sub(pattern, replacement, content, flags=re.DOTALL)

    with open("README.md", "w", encoding="utf-8") as f:
        f.write(new_content)
    print("âœ¨ README å·²æˆåŠŸæ›´æ–°ï¼")

if __name__ == "__main__":
    image_list = get_xiaodouni_images()
    update_readme(image_list)
