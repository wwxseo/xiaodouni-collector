import os
import re
import requests
import random
from bs4 import BeautifulSoup
from datetime import datetime

# æœç´¢å…³é”®è¯
QUERIES = ["å°è±†æ³¥ é«˜æ¸…", "å°è±†æ³¥ wallpaper", "å°è±†æ³¥ funny bean", "å°è±†æ³¥ æ’ç”»"]
# åŸŸåé»‘åå•ï¼šè¿™äº›ç½‘ç«™çš„å›¾æå…¶å®¹æ˜“è£‚å¼€ï¼Œå³ä¾¿æœ‰ä»£ç†ä¹Ÿéš¾æ•‘ï¼Œç›´æ¥è·³è¿‡
DOMAIN_BLACKLIST = ["baidu.com", "weibo.com", "sinaimg.cn", "zhimg.com", "csdnimg.cn"]

def get_seen_urls():
    seen = set()
    if os.path.exists("history.md"):
        with open("history.md", "r", encoding="utf-8") as f:
            # æå–åŸå§‹é“¾æ¥ï¼Œæ’é™¤ä»£ç†å‰ç¼€
            urls = re.findall(r'url=(http[^"\'&\s]+)', f.read())
            for u in urls:
                seen.add(u)
    print(f"ğŸ“œ è®°å¿†åº“å·²åŠ è½½: {len(seen)} å¼ å†å²å›¾ç‰‡ã€‚")
    return seen

def wrap_proxy(url):
    """ä½¿ç”¨ wsrv.nl ä»£ç†å›¾ç‰‡ï¼Œç»•è¿‡é˜²ç›—é“¾ï¼Œå¼ºåˆ¶è½¬æ¢æ ¼å¼å¹¶æ·»åŠ ç™½è‰²èƒŒæ™¯"""
    # &bg=white: å¤„ç†é€æ˜PNGå˜é»‘çš„é—®é¢˜
    # &we: ç»•è¿‡æŸäº›é˜²ç›—é“¾é”™è¯¯
    return f"https://wsrv.nl/?url={url}&bg=white"

def is_valid(url, seen_urls):
    """æ£€æŸ¥å›¾ç‰‡é“¾æ¥æ˜¯å¦æœ‰æ•ˆä¸”ä¸åœ¨é»‘åå•"""
    if not url.startswith("http"): return False
    if url in seen_urls: return False
    if any(bad in url for bad in DOMAIN_BLACKLIST): return False
    # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾çš„è¡¨æƒ…åŒ…å°å›¾é“¾æ¥
    if any(x in url.lower() for x in ["/100/100", "avatar", "icon"]): return False
    return True

def fetch_from_bing(query, seen_urls, needed):
    print(f"ğŸ” Bing æœç´¢: {query}...")
    images = []
    url = f"https://www.bing.com/images/search?q={query}&qft=+filterui:imagesize-large&form=IRFLTR"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        for a in soup.find_all("a", class_="iusc"):
            m = a.get("m")
            if m:
                murl = re.search(r'"murl":"(.*?)"', m)
                if murl:
                    link = murl.group(1)
                    if is_valid(link, seen_urls):
                        images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

def fetch_from_sogou(query, seen_urls, needed):
    print(f"ğŸ” æœç‹—å›¾ç‰‡æœç´¢: {query}...")
    images = []
    url = f"https://pic.sogou.com/pics?query={query}"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        all_urls = re.findall(r'https?://[^"\'\s]+\.(?:jpg|jpeg|png)', resp.text)
        for link in all_urls:
            if "sogou.com" not in link and is_valid(link, seen_urls):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

def get_all_images():
    seen = get_seen_urls()
    final_images = []
    fetchers = [fetch_from_bing, fetch_from_sogou]
    random.shuffle(fetchers)
    
    for fetcher in fetchers:
        needed = 12 - len(final_images)
        if needed <= 0: break
        query = random.choice(QUERIES)
        new_batch = fetcher(query, seen, needed)
        final_images.extend(new_batch)
    
    # å°†æŠ“åˆ°çš„åŸå§‹é“¾æ¥å…¨éƒ¨åŒ…è£…ä¸Šä»£ç†
    proxied_images = [wrap_proxy(img) for img in final_images]
    print(f"âœ… æœ€ç»ˆè·å–äº† {len(proxied_images)} å¼ é€šè¿‡ä»£ç†åŒ…è£…çš„æ–°å›¾")
    return proxied_images

def update_files(urls):
    if not urls: return
    # 1. æ›´æ–° README.md
    with open("README.md", "r", encoding="utf-8") as f:
        content = f.read()
    
    img_html = '<div align="center">\n'
    for url in urls:
        img_html += f'  <img src="{url}" width="180" alt="å°è±†æ³¥" style="margin:5px; border-radius:12px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">'
    img_html += '\n  <p><i>ğŸ”„ æ™ºèƒ½ä»£ç†åŠ é€Ÿä¸­ï¼Œè®©å¯çˆ±æ°¸ä¸æ‰çº¿</i></p>\n</div>'

    pattern = r"<!-- START_SECTION:xiaodouni -->.*?<!-- END_SECTION:xiaodouni -->"
    replacement = f"<!-- START_SECTION:xiaodouni -->\n{img_html}\n<!-- END_SECTION:xiaodouni -->"
    new_readme = re.sub(pattern, replacement, content, flags=re.DOTALL) if "<!-- START_SECTION:xiaodouni -->" in content else content + "\n\n" + replacement
    
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(new_readme)

    # 2. æ›´æ–° history.md
    today = datetime.now().strftime("%Y-%m-%d")
    if not os.path.exists("history.md"):
        with open("history.md", "w", encoding="utf-8") as f:
            f.write("# ğŸ“š å°è±†æ³¥å†å²æ”¶è—é¦†\n\n---\n")
            
    with open("history.md", "a", encoding="utf-8") as f:
        f.write(f"\n### ğŸ“… {today}\n<div align='left'>\n")
        for url in urls:
            f.write(f'  <img src="{url}" width="120" style="margin:2px; border-radius:5px;">\n')
        f.write("</div>\n\n---\n")

if __name__ == "__main__":
    imgs = get_all_images()
    update_files(imgs)
