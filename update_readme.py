import os
import re
import requests
import random
from bs4 import BeautifulSoup
from datetime import datetime

# ç²¾å‡†å…³é”®è¯ï¼Œé¿å¼€å®ç‰©è±†å­
QUERIES = ["å°è±†æ³¥ æ¼«ç”»", "å°è±†æ³¥ è¡¨æƒ…åŒ…", "å°è±†æ³¥ funny bean cat", "å°è±†æ³¥ åŠ¨ç”»", "å°è±†æ³¥ åŠ¨æ€å›¾"]
DOMAIN_BLACKLIST = ["baidu.com", "weibo.com", "sinaimg.cn", "zhimg.com", "csdnimg.cn", "funnybean.com"]

def get_seen_urls():
    seen = set()
    if os.path.exists("history.md"):
        with open("history.md", "r", encoding="utf-8") as f:
            urls = re.findall(r'url=(http[^"\'&\s]+)', f.read())
            for u in urls:
                seen.add(u)
    return seen

def wrap_proxy(url):
    """ä½¿ç”¨ä»£ç†å¹¶å¼ºåˆ¶ç¼©æ”¾è£å‰ªæˆæ­£æ–¹å½¢ï¼Œè®©æ’ç‰ˆæåº¦æ•´é½"""
    # w=200&h=200&fit=cover: å¼ºåˆ¶è£å‰ªæˆ 200x200 çš„æ­£æ–¹å½¢
    return f"https://wsrv.nl/?url={url}&w=200&h=200&fit=cover&bg=white"

def is_valid_image(url, seen_urls, session_images):
    """éªŒè¯å›¾ç‰‡æ˜¯å¦çœŸçš„å­˜åœ¨ï¼Œä¸”ä¸æ˜¯åå›¾"""
    if not url.startswith("http"): return False
    if any(bad in url for bad in DOMAIN_BLACKLIST): return False
    if url in seen_urls or url in session_images: return False
    
    try:
        # å‘é€ä¸€ä¸ª HEAD è¯·æ±‚ï¼Œåªæ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆï¼Œä¸ä¸‹è½½å›¾ç‰‡ï¼Œé€Ÿåº¦æå¿«
        res = requests.head(url, timeout=3, allow_redirects=True)
        if res.status_code == 200 and int(res.headers.get('Content-Length', 0)) > 5000:
            return True
    except:
        return False
    return False

def fetch_images(query, seen_urls, session_images, needed):
    print(f"ğŸ” æ­£åœ¨ä»å¤šæºæœå¯» '{query}'...")
    images = []
    # è½®æµå°è¯• Bing å’Œ 360
    urls = [
        f"https://www.bing.com/images/search?q={query}&form=HDRSC3",
        f"https://image.so.com/i?q={query}"
    ]
    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    
    for url in urls:
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            # å…¼å®¹ Bing çš„åŒ¹é…
            murls = re.findall(r'"murl":"(.*?)"', resp.text)
            # å…¼å®¹ 360/æœç‹— çš„åŒ¹é…
            others = re.findall(r'https?://[^"\'\s]+\.(?:jpg|jpeg|png)', resp.text)
            
            for link in (murls + others):
                if is_valid_image(link, seen_urls, session_images + images):
                    images.append(link)
                    print(f"âœ… æ‰¾åˆ°ä¸€å¼ æœ‰æ•ˆæ–°å›¾: {link[:50]}...")
                if len(images) >= needed: return images
        except: continue
    return images

def get_all_images():
    seen = get_seen_urls()
    final_images = []
    
    # å°è¯•å¤šæ¬¡ï¼Œç›´åˆ°å‡‘æ»¡ 12 å¼ 
    attempts = 0
    while len(final_images) < 12 and attempts < 5:
        query = random.choice(QUERIES)
        needed = 12 - len(final_images)
        batch = fetch_images(query, seen, final_images, needed)
        final_images.extend(batch)
        attempts += 1
    
    return [wrap_proxy(img) for img in final_images]

def update_files(urls):
    if not urls: return
    with open("README.md", "r", encoding="utf-8") as f:
        content = f.read()
    
    # å¼ºåˆ¶æ­£æ–¹å½¢ç½‘æ ¼æ’ç‰ˆ
    img_html = '<div align="center">\n'
    for url in urls:
        # ç§»é™¤äº†æ¢è¡Œï¼Œå¢åŠ ç­‰é«˜å®½æ§åˆ¶
        img_html += f'  <img src="{url}" width="160" height="160" alt="å°è±†æ³¥" style="margin:2px; border-radius:8px; object-fit:cover;">'
    img_html += '\n  <p><i>ğŸ”„ æ™ºèƒ½è¿‡æ»¤ & è‡ªåŠ¨è£å‰ªï¼Œæ¯æ—¥å‘ç°é«˜æ¸…å°è±†æ³¥</i></p>\n</div>'

    pattern = r"<!-- START_SECTION:xiaodouni -->.*?<!-- END_SECTION:xiaodouni -->"
    replacement = f"<!-- START_SECTION:xiaodouni -->\n{img_html}\n<!-- END_SECTION:xiaodouni -->"
    new_readme = re.sub(pattern, replacement, content, flags=re.DOTALL)
    
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(new_readme)

    # æ›´æ–° history.md
    today = datetime.now().strftime("%Y-%m-%d")
    if not os.path.exists("history.md"):
        with open("history.md", "w", encoding="utf-8") as f:
            f.write("# ğŸ“š å°è±†æ³¥å†å²æ”¶è—é¦†\n\n---\n")
            
    with open("history.md", "a", encoding="utf-8") as f:
        f.write(f"\n### ğŸ“… {today}\n<div align='left'>\n")
        for url in urls:
            f.write(f'  <img src="{url}" width="100" height="100" style="margin:2px; border-radius:5px; object-fit:cover;">\n')
        f.write("</div>\n\n---\n")

if __name__ == "__main__":
    imgs = get_all_images()
    update_files(imgs)
