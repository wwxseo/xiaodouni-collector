import os
import re
import requests
import random
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import quote  # å…³é”®ï¼šç”¨äºä¿®å¤ Not Found é—®é¢˜

# --- 1. é…ç½®åŒºåŸŸ ---

# å…³é”®è¯ï¼šåŠ ä¸Š cat/comic ç¡®ä¿æœåˆ°çŒ«
QUERIES = ["å°è±†æ³¥ cat", "å°è±†æ³¥ funny bean", "å°è±†æ³¥ æ¼«ç”»", "å°è±†æ³¥ è¡¨æƒ…åŒ…", "å°è±†æ³¥ åŠ¨ç”»", "å°è±†æ³¥ å£çº¸"]

# ä¸¥æ ¼é»‘åå•ï¼šç¦æ­¢æœç´¢å¼•æ“è‡ªå®¶åŸŸåï¼Œç¦æ­¢æµæ°“å›¾æº
ENGINE_DOMAINS = ["bing.com", "sogou.com", "so.com", "qhimg.com", "qhimgs.com", "baidu.com"]
DOMAIN_BLACKLIST = ["weibo.com", "sinaimg.cn", "zhimg.com", "csdnimg.cn", "127.net"]

# --- 2. å·¥å…·å‡½æ•° ---

def clean_url(url):
    """ã€å…³é”®ä¿®å¤ã€‘æ¸…æ´— URL ä¸­çš„è½¬ä¹‰å­—ç¬¦"""
    if not url: return ""
    # ä¿®å¤ JSON ä¸­çš„è½¬ä¹‰æ–œæ  https:\/\/ -> https://
    url = url.replace(r'\/', '/')
    # ä¿®å¤ unicode è½¬ä¹‰ç¬¦
    try:
        url = url.encode('utf-8').decode('unicode_escape')
    except:
        pass
    # ä¿®å¤ HTML å®ä½“
    url = url.replace('&amp;', '&')
    return url

def wrap_proxy(url):
    """ã€ä¿®å¤æ ¸å¿ƒã€‘å¯¹ URL è¿›è¡Œç¼–ç ï¼Œé˜²æ­¢ Not Foundï¼Œå¹¶å¼ºåˆ¶è£å‰ª"""
    clean = clean_url(url)
    # quote å°†é“¾æ¥ä¸­çš„ & ? ç­‰ç¬¦å·è½¬ä¹‰ï¼Œç¡®ä¿ä»£ç†æœåŠ¡å™¨èƒ½è¯»æ‡‚å®Œæ•´é“¾æ¥
    encoded_url = quote(clean, safe='')
    # w=300&h=300&fit=cover: å¼ºåˆ¶æ­£æ–¹å½¢è£å‰ª
    # bg=white: ä¿®å¤é€æ˜å›¾å˜é»‘
    # output=jpg: ç»Ÿä¸€æ ¼å¼
    return f"https://wsrv.nl/?url={encoded_url}&w=300&h=300&fit=cover&bg=white&output=jpg"

def is_valid(url, seen_urls, session_images):
    """è¶…çº§ä¸¥æ ¼çš„è¿‡æ»¤å™¨"""
    if not url: return False
    url_lower = url.lower()
    
    if not url.startswith("http"): return False
    
    # 1. æ€æ‰æœç´¢å¼•æ“è‡ªå®¶çš„å›¾ (Bing Logo æ€æ‰‹)
    if any(engine in url_lower for engine in ENGINE_DOMAINS): return False
    
    # 2. æ€æ‰åå›¾æº
    if any(bad in url_lower for bad in DOMAIN_BLACKLIST): return False
    
    # 3. å»é‡ (å†å² + æœ¬æ¬¡)
    if url in seen_urls or url in session_images: return False
    
    # 4. æ€æ‰ UI å›¾æ ‡
    if any(x in url_lower for x in ["logo", "icon", "avatar", "sign", "symbol", "loading", "gif"]): return False
    
    # 5. ç®€å•æ£€æŸ¥åç¼€ (æ”¾å®½ä¸€ç‚¹ï¼Œå› ä¸ºæœ‰äº›å›¾åºŠæ²¡åç¼€)
    if not any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.gif', 'webp', 'me']):
        if 'http' not in url_lower[4:]: 
            pass # å¯èƒ½æ˜¯åŠ¨æ€é“¾æ¥
        else:
            return False

    return True

def get_seen_urls():
    """ä»å†å²è®°å½•æå– URLï¼Œç”¨äºå»é‡"""
    seen = set()
    if os.path.exists("history.md"):
        try:
            with open("history.md", "r", encoding="utf-8") as f:
                content = f.read()
                urls = re.findall(r'url=(http[^"\'&\s]+)', content)
                for u in urls:
                    seen.add(u)
        except Exception:
            pass
    print(f"ğŸ“œ è®°å¿†åº“åŠ è½½: {len(seen)} å¼ å†å²å›¾ç‰‡ã€‚")
    return seen

# --- 3. æŠ“å–é€»è¾‘ (ä¸‰æº) ---

def fetch_from_bing(query, seen_urls, session_images, needed):
    print(f"ğŸ” [Bing] æœ: {query}")
    images = []
    # éšæœºåç§»ï¼Œé¿å¼€é¦–ä½å¹¿å‘Š
    first = random.randint(1, 10)
    url = f"https://www.bing.com/images/search?q={query}&first={first}&safeSearch=Moderate"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        # æå– murl
        links = re.findall(r'"murl":"(.*?)"', resp.text)
        for link in links:
            link = clean_url(link)
            if is_valid(link, seen_urls, session_images + images):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

def fetch_from_sogou(query, seen_urls, session_images, needed):
    print(f"ğŸ” [Sogou] æœ: {query}")
    images = []
    url = f"https://pic.sogou.com/pics?query={query}"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        # ä¼˜å…ˆæ‰¾ thumbUrl
        links = re.findall(r'"thumbUrl":"(http[^"]+)"', resp.text)
        if not links:
            links = re.findall(r'https?://[^"\'\s]+\.(?:jpg|jpeg|png)', resp.text)
        for link in links:
            link = clean_url(link)
            if is_valid(link, seen_urls, session_images + images):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

def fetch_from_360(query, seen_urls, session_images, needed):
    print(f"ğŸ” [360] æœ: {query}")
    images = []
    url = f"https://image.so.com/i?q={query}"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        links = re.findall(r'"img":"(http[^"]+)"', resp.text)
        if not links:
            links = re.findall(r'https?://[^"\'\s]+\.(?:jpg|jpeg|png)', resp.text)
        for link in links:
            link = clean_url(link)
            if is_valid(link, seen_urls, session_images + images):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

# --- 4. ä¸»æµç¨‹ ---

def get_all_images():
    seen = get_seen_urls()
    final_images = []
    
    # éšæœºæ‰“ä¹±æºé¡ºåº
    fetchers = [fetch_from_bing, fetch_from_sogou, fetch_from_360]
    random.shuffle(fetchers)
    
    # å°è¯•å‡‘é½ 12 å¼ 
    for fetcher in fetchers:
        needed = 12 - len(final_images)
        if needed <= 0: break
        
        query = random.choice(QUERIES)
        new_batch = fetcher(query, seen, final_images, needed)
        final_images.extend(new_batch)
    
    # ä¿åº•æœºåˆ¶ï¼šå¦‚æœæ²¡å‡‘é½ï¼Œæ¢è¯å†è¯•ä¸€æ¬¡
    if len(final_images) < 12:
        print("ğŸ’¡ æ•°é‡ä¸è¶³ï¼Œå¯åŠ¨äºŒè½®è¡¥è´§...")
        for q in QUERIES:
            if len(final_images) >= 12: break
            # é»˜è®¤ç”¨ Bing è¡¥è´§
            final_images.extend(fetch_from_bing(q, seen, final_images, 12 - len(final_images)))

    print(f"ğŸ¯ æœ¬æ¬¡æ•è· {len(final_images)} å¼ å›¾ç‰‡")
    # è¿™é‡Œä¸éœ€è¦å† quote äº†ï¼Œå› ä¸º wrap_proxy å†…éƒ¨å·²ç»å¤„ç†äº†
    return [wrap_proxy(img) for img in final_images]

def update_files(urls):
    if not urls:
        print("âŒ æ— å›¾ï¼Œç»“æŸã€‚")
        return

    # æ„å»º HTML (ä½¿ç”¨ 300x300 çš„å›¾æºï¼Œé¡µé¢æ˜¾ç¤º 160x160)
    img_html = '<div align="center">\n'
    for url in urls:
        img_html += f'  <img src="{url}" width="160" height="160" alt="å°è±†æ³¥" style="margin:4px; border-radius:12px; object-fit:cover; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">'
    img_html += '\n  <p><i>ğŸ± æ¯æ—¥éšæœºä¸‰æºæœç½—ï¼Œåªé€‰é«˜æ¸…çŒ«çŒ«å¤´</i></p>\n</div>'

    # ä½¿ç”¨å­—ç¬¦ä¸²æ‹¼æ¥æ¨¡å¼æ›´æ–° README (é˜²æ­¢ Bad Escape é”™è¯¯)
    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
            
        start_marker = "<!-- START_SECTION:xiaodouni -->"
        end_marker = "<!-- END_SECTION:xiaodouni -->"
        
        if start_marker in content and end_marker in content:
            start_idx = content.find(start_marker) + len(start_marker)
            end_idx = content.find(end_marker)
            new_content = content[:start_idx] + "\n" + img_html + "\n" + content[end_idx:]
            
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(new_content)
            print("âœ… README æ›´æ–°æˆåŠŸ")
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°æ ‡è®°ï¼Œåˆ™æ–°å»ºæˆ–è¿½åŠ 
            new_content = f"# å°è±†æ³¥æ”¶é›†å™¨\n\n{start_marker}\n{img_html}\n{end_marker}"
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(new_content)
            print("âš ï¸ é‡æ–°åˆå§‹åŒ–äº† README")

    # æ›´æ–° history.md
    today = datetime.now().strftime("%Y-%m-%d")
    if not os.path.exists("history.md"):
        with open("history.md", "w", encoding="utf-8") as f:
            f.write("# ğŸ“š å°è±†æ³¥å†å²æ”¶è—é¦†\n\n---\n")
            
    with open("history.md", "a", encoding="utf-8") as f:
        f.write(f"\n### ğŸ“… {today}\n<div align='left'>\n")
        for url in urls:
            f.write(f'  <img src="{url}" width="100" height="100" style="margin:2px; border-radius:6px; object-fit:cover;">\n')
        f.write("</div>\n\n---\n")
    print("âœ… History å½’æ¡£æˆåŠŸ")

if __name__ == "__main__":
    imgs = get_all_images()
    update_files(imgs)
