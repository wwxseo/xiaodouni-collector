import os
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime

def get_xiaodouni_images():
    print("ğŸš€ å¼€å§‹æœå¯»é«˜æ¸…å°è±†æ³¥...")
    search_urls = [
        "https://www.bing.com/images/search?q=å°è±†æ³¥+wallpaper&qft=+filterui:imagesize-large&form=IRFLTR",
        "https://www.bing.com/images/search?q=å°è±†æ³¥"
    ]
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    images = []
    for url in search_urls:
        if len(images) >= 12: break
        try:
            response = requests.get(url, headers=headers, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            for img_tag in soup.find_all("a", class_="iusc"):
                m_content = img_tag.get("m")
                if m_content:
                    murl_match = re.search(r'"murl":"(.*?)"', m_content)
                    if murl_match:
                        img_url = murl_match.group(1)
                        if img_url.startswith("http") and img_url not in images:
                            images.append(img_url)
                if len(images) >= 12: break
        except: continue
    print(f"ğŸ¯ æœ€ç»ˆæ•è·å°è±†æ³¥æ•°é‡: {len(images)}")
    return images

def update_readme(urls):
    """æ›´æ–°é¦–é¡µ README.md"""
    if not urls: return
    if not os.path.exists("README.md"): return
    
    with open("README.md", "r", encoding="utf-8") as f:
        content = f.read()

    img_html = '<div align="center">\n'
    for url in urls:
        img_html += f'  <img src="{url}" width="180" alt="å°è±†æ³¥" style="margin:5px; border-radius:12px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">'
    img_html += '\n  <p><i>ğŸ”„ æ¯æ—¥è‡ªåŠ¨æ›´æ–°ï¼Œæœé›†è‡ªå…¨ç½‘é«˜æ¸…å›¾æº</i></p>\n</div>'
    
    start_tag = "<!-- START_SECTION:xiaodouni -->"
    end_tag = "<!-- END_SECTION:xiaodouni -->"
    
    if start_tag in content and end_tag in content:
        pattern = r"<!-- START_SECTION:xiaodouni -->.*?<!-- END_SECTION:xiaodouni -->"
        replacement = f"{start_tag}\n{img_html}\n{end_tag}"
        new_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
    else:
        new_content = content + f"\n\n{start_tag}\n{img_html}\n{end_tag}"

    with open("README.md", "w", encoding="utf-8") as f:
        f.write(new_content)
    print("âœ¨ README å·²æ›´æ–°ï¼")

def update_history(urls):
    """è¿½åŠ åˆ° history.md"""
    if not urls: return
    
    # è·å–å½“å‰æ—¥æœŸ (ä¾‹å¦‚: 2025-05-20)
    today = datetime.now().strftime("%Y-%m-%d")
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå…ˆå†™ä¸ªæ ‡é¢˜
    if not os.path.exists("history.md"):
        with open("history.md", "w", encoding="utf-8") as f:
            f.write("# ğŸ“š å°è±†æ³¥å†å²æ”¶è—é¦†\n\nè¿™é‡Œè®°å½•äº†è‡ªæœ¬é¡¹ç›®å¯åŠ¨ä»¥æ¥æŠ“å–è¿‡çš„æ‰€æœ‰å›¾ç‰‡ã€‚\n\n---\n")

    # è¿½åŠ æ–°æŠ“åˆ°çš„å›¾ç‰‡ï¼ˆä½¿ç”¨å°ç¼©ç•¥å›¾ï¼Œé˜²æ­¢é¡µé¢å¤ªé•¿ï¼‰
    with open("history.md", "a", encoding="utf-8") as f:
        f.write(f"\n### ğŸ“… {today}\n")
        f.write('<div align="left">\n')
        for url in urls:
            f.write(f'  <img src="{url}" width="120" style="margin:2px; border-radius:5px;">\n')
        f.write('</div>\n\n---\n')
    print(f"ğŸ“– å·²æˆåŠŸå½’æ¡£ {len(urls)} å¼ å›¾ç‰‡åˆ° history.md")

if __name__ == "__main__":
    image_list = get_xiaodouni_images()
    update_readme(image_list)
    update_history(image_list)
