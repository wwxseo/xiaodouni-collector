import os
import re
import requests
import random
from bs4 import BeautifulSoup
from datetime import datetime

def get_xiaodouni_images():
    print("ğŸš€ å¼€å¯éšæœºæ¢ç´¢æ¨¡å¼ï¼Œæœå¯»æ–°é²œçš„å°è±†æ³¥...")
    # å¢åŠ å‡ ä¸ªä¸åŒçš„æœç´¢è¯ï¼Œæ¯æ¬¡è¿è¡Œéšæœºé€‰ä¸€ä¸ªï¼Œå¢åŠ å¤šæ ·æ€§
    queries = ["å°è±†æ³¥ é«˜æ¸…", "å°è±†æ³¥ å£çº¸", "å°è±†æ³¥ æ’ç”»", "å°è±†æ³¥ funny bean"]
    selected_query = random.choice(queries)
    
    url = f"https://www.bing.com/images/search?q={selected_query}&qft=+filterui:imagesize-large&form=IRFLTR"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        pool = []
        for img_tag in soup.find_all("a", class_="iusc"):
            m_content = img_tag.get("m")
            if m_content:
                murl_match = re.search(r'"murl":"(.*?)"', m_content)
                if murl_match:
                    img_url = murl_match.group(1)
                    if img_url.startswith("http"):
                        pool.append(img_url)
        
        # ä»æœåˆ°çš„å‡ åå¼ å›¾ä¸­éšæœºæŠ½å– 12 å¼ 
        sample_size = min(len(pool), 12)
        images = random.sample(pool, sample_size)
        
        print(f"ğŸ¯ ä» {len(pool)} å¼ å€™é€‰å›¾ä¸­éšæœºé€‰ä¸­äº† {len(images)} å¼ ")
        return images
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return []

def update_readme(urls):
    if not urls: return
    with open("README.md", "r", encoding="utf-8") as f:
        content = f.read()

    img_html = '<div align="center">\n'
    for url in urls:
        img_html += f'  <img src="{url}" width="180" alt="å°è±†æ³¥" style="margin:5px; border-radius:12px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">'
    img_html += f'\n  <p><i>ğŸ”„ æ¯æ—¥éšæœºæ›´æ–°ï¼Œå½“å‰ä¸»é¢˜ï¼šé«˜æ¸…æœç½—</i></p>\n</div>'
    
    start_tag = "<!-- START_SECTION:xiaodouni -->"
    end_tag = "<!-- END_SECTION:xiaodouni -->"
    
    if start_tag in content and end_tag in content:
        pattern = r"<!-- START_SECTION:xiaodouni -->.*?<!-- END_SECTION:xiaodouni -->"
        replacement = f"{start_tag}\n{img_html}\n{end_tag}"
        new_content = re.sub(pattern, replacement, content, flags=re.DOTALL)
        with open("README.md", "w", encoding="utf-8") as f:
            f.write(new_content)
        print("âœ¨ README å·²æ›´æ–°ï¼")

def update_history(urls):
    if not urls: return
    today = datetime.now().strftime("%Y-%m-%d")
    
    # æ£€æŸ¥æ˜¯å¦ä»Šå¤©å·²ç»è®°å½•è¿‡äº†ï¼Œé¿å…é‡å¤è¿è¡Œå¯¼è‡´çš„ä¸€å¤©å¤šæ¡
    if os.path.exists("history.md"):
        with open("history.md", "r", encoding="utf-8") as f:
            if f"### ğŸ“… {today}" in f.read():
                # å¦‚æœä½ æƒ³ä¸€å¤©å­˜å¤šä»½ï¼Œå°±æŠŠä¸‹é¢è¿™è¡Œåˆ æ‰
                print("ğŸ“… ä»Šå¤©å·²ç»å½’æ¡£è¿‡äº†ï¼Œä¸ºäº†ä¿æŒç®€æ´ï¼Œä¸å†é‡å¤æ·»åŠ ã€‚")
                return

    if not os.path.exists("history.md"):
        with open("history.md", "w", encoding="utf-8") as f:
            f.write("# ğŸ“š å°è±†æ³¥å†å²æ”¶è—é¦†\n\n---\n")

    with open("history.md", "a", encoding="utf-8") as f:
        f.write(f"\n### ğŸ“… {today}\n")
        f.write('<div align="left">\n')
        for url in urls:
            f.write(f'  <img src="{url}" width="120" style="margin:2px; border-radius:5px;">\n')
        f.write('</div>\n\n---\n')
    print("ğŸ“– å·²æˆåŠŸå½’æ¡£åˆ° history.md")

if __name__ == "__main__":
    image_list = get_xiaodouni_images()
    update_readme(image_list)
    update_history(image_list)
