import os
import re
import requests
import random
from bs4 import BeautifulSoup
from datetime import datetime

# 1. é…ç½®ï¼šç²¾å‡†å…³é”®è¯ï¼Œé¿å¼€å®ç‰©è±†å­
QUERIES = ["å°è±†æ³¥ cat", "å°è±†æ³¥ funny bean", "å°è±†æ³¥ æ¼«ç”»", "å°è±†æ³¥ è¡¨æƒ…åŒ…", "å°è±†æ³¥ åŠ¨ç”»"]
# 2. é…ç½®ï¼šæ›´å¼ºçš„è¿‡æ»¤é»‘åå•ï¼ˆé¿å¼€æµæ°“æºå’Œæœç´¢ UIï¼‰
DOMAIN_BLACKLIST = [
    "bing.com/th", "bing.com/sa", "baidu.com", "weibo.com", 
    "sinaimg.cn", "zhimg.com", "csdnimg.cn", "so.com", "sogou.com"
]

def get_seen_urls():
    """ã€åŠŸèƒ½ï¼šå†å²å»é‡ã€‘ä» history.md ä¸­æå–åŸå§‹æŠ“å–è¿‡çš„ URL"""
    seen = set()
    if os.path.exists("history.md"):
        with open("history.md", "r", encoding="utf-8") as f:
            content = f.read()
            # æå–ä»£ç†é“¾æ¥ä¸­çš„åŸå§‹é“¾æ¥éƒ¨åˆ† (url=... ä¹‹åçš„å†…å®¹)
            urls = re.findall(r'url=(http[^"\'&\s]+)', content)
            for u in urls:
                seen.add(u)
    print(f"ğŸ“œ è®°å¿†åº“åŠ è½½å®Œæˆ: å·²è®°å½• {len(seen)} å¼ å†å²å›¾ç‰‡ã€‚")
    return seen

def wrap_proxy(url):
    """ã€åŠŸèƒ½ï¼šä»£ç†ä¿æŠ¤+é˜²é»‘å—ã€‘é˜²ç›—é“¾å…‹æ˜Ÿï¼Œå¼ºåˆ¶è£å‰ªå¹¶åŠ ç™½åº•"""
    # w=300&h=300&fit=cover: å¼ºåˆ¶è£å‰ªä¸ºç­‰å¤§æ­£æ–¹å½¢
    # bg=white: è§£å†³é€æ˜PNGèƒŒæ™¯å˜é»‘çš„é—®é¢˜
    return f"https://wsrv.nl/?url={url}&w=300&h=300&fit=cover&bg=white"

def is_valid(url, seen_urls, session_images):
    """ã€åŠŸèƒ½ï¼šå¤šé‡å»é‡+ç²¾å‡†æ’é™¤ã€‘æ’é™¤é»‘åå•ã€å†å²é‡å¤ã€æœ¬æ¬¡é‡å¤"""
    if not url.startswith("http"): return False
    if any(bad in url for bad in DOMAIN_BLACKLIST): return False
    if url in seen_urls or url in session_images: return False
    # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾çš„ UI å›¾æ ‡æˆ–ä½è´¨å›¾æºæ ‡å¿—
    if any(x in url.lower() for x in ["/100/100", "avatar", "icon", "logo", "thumbnail"]): return False
    return True

# --- ä¸‰å¤§å¼•æ“æŠ“å–å‡½æ•° ---

def fetch_from_bing(query, seen_urls, session_images, needed):
    print(f"ğŸ” [Source: Bing] æœç´¢: {query}...")
    images = []
    url = f"https://www.bing.com/images/search?q={query}&safeSearch=Moderate"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        # Bing çš„å›¾ç‰‡åœ°å€è—åœ¨ murl å­—æ®µ
        links = re.findall(r'"murl":"(.*?)"', resp.text)
        for link in links:
            if is_valid(link, seen_urls, session_images + images):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

def fetch_from_sogou(query, seen_urls, session_images, needed):
    print(f"ğŸ” [Source: Sogou] æœç´¢: {query}...")
    images = []
    url = f"https://pic.sogou.com/pics?query={query}"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        # æœç‹—å›¾ç‰‡åœ°å€åŒ¹é…
        links = re.findall(r'https?://[^"\'\s]+\.(?:jpg|jpeg|png)', resp.text)
        for link in links:
            if is_valid(link, seen_urls, session_images + images):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

def fetch_from_360(query, seen_urls, session_images, needed):
    print(f"ğŸ” [Source: 360] æœç´¢: {query}...")
    images = []
    url = f"https://image.so.com/i?q={query}"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        # 360å›¾ç‰‡åœ°å€åŒ¹é…
        links = re.findall(r'https?://[^"\'\s]+\.(?:jpg|jpeg|png)', resp.text)
        for link in links:
            if is_valid(link, seen_urls, session_images + images):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

# --- ä¸»é€»è¾‘ ---

def get_all_images():
    seen = get_seen_urls()
    final_images = []
    
    # 1. ã€åŠŸèƒ½ï¼šæºé¡ºåºéšæœºåŒ–ã€‘
    fetchers = [fetch_from_bing, fetch_from_sogou, fetch_from_360]
    random.shuffle(fetchers)
    
    # 2. ã€åŠŸèƒ½ï¼šå¼ºåˆ¶ 12 å¼  & æé«˜å®¹é”™æ€§ã€‘
    # å³ä½¿ä¸€ä¸ªå¼•æ“ä¸è¡Œï¼Œä¹Ÿä¼šè‡ªåŠ¨åˆ‡ä¸‹ä¸€ä¸ªï¼Œç›´åˆ°å‡‘é½
    for fetcher in fetchers:
        current_needed = 12 - len(final_images)
        if current_needed <= 0: break
        
        # æ¯æ¬¡ä»å…³é”®è¯åº“éšæœºæŠ½ä¸€ä¸ªï¼Œå¢åŠ å¤šæ ·æ€§
        query = random.choice(QUERIES)
        new_batch = fetcher(query, seen, final_images, current_needed)
        final_images.extend(new_batch)
    
    print(f"ğŸ¯ æœ¬æ¬¡ä»»åŠ¡å…±æˆåŠŸæ•è· {len(final_images)} å¼ å…¨æ–°å°è±†æ³¥ã€‚")
    # 3. ã€åŠŸèƒ½ï¼šä»£ç†åŒ…è£…ã€‘ç»Ÿä¸€å¤„ç†é˜²ç›—é“¾å’Œé»‘è‰²å—
    return [wrap_proxy(img) for img in final_images]

def update_files(urls):
    if not urls:
        print("âŒ æœªæ•è·åˆ°æ–°å›¾ï¼Œè·³è¿‡æ›´æ–°ã€‚")
        return

    # A. æ›´æ–° README.md (ç²¾å‡†æ’ç‰ˆä¼˜åŒ–)
    with open("README.md", "r", encoding="utf-8") as f:
        content = f.read()
    
    img_html = '<div align="center">\n'
    for url in urls:
        # ä½¿ç”¨ 160x160 ç»Ÿä¸€ç½‘æ ¼å¸ƒå±€
        img_html += f'  <img src="{url}" width="160" height="160" alt="å°è±†æ³¥" style="margin:4px; border-radius:12px; object-fit:cover; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">'
    img_html += '\n  <p><i>ğŸ± ä¸‰æºè”æœ & æ™ºèƒ½è£å‰ªä»£ç†ï¼Œè®©å¯çˆ±æ°¸ä¸æ‰çº¿</i></p>\n</div>'

    pattern = r"<!-- START_SECTION:xiaodouni -->.*?<!-- END_SECTION:xiaodouni -->"
    replacement = f"<!-- START_SECTION:xiaodouni -->\n{img_html}\n<!-- END_SECTION:xiaodouni -->"
    
    if "<!-- START_SECTION:xiaodouni -->" in content:
        new_readme = re.sub(pattern, replacement, content, flags=re.DOTALL)
        with open("README.md", "w", encoding="utf-8") as f:
            f.write(new_readme)

    # B. æ›´æ–° history.md (å½’æ¡£ä¿å­˜)
    today = datetime.now().strftime("%Y-%m-%d")
    if not os.path.exists("history.md"):
        with open("history.md", "w", encoding="utf-8") as f:
            f.write("# ğŸ“š å°è±†æ³¥å†å²æ”¶è—é¦†\n\n---\n")
            
    with open("history.md", "a", encoding="utf-8") as f:
        f.write(f"\n### ğŸ“… {today}\n<div align='left'>\n")
        for url in urls:
            f.write(f'  <img src="{url}" width="100" height="100" style="margin:2px; border-radius:6px; object-fit:cover;">\n')
        f.write("</div>\n\n---\n")
    print("âœ¨ ä»»åŠ¡æˆåŠŸï¼README ä¸ history.md å·²åŒæ­¥ã€‚")

if __name__ == "__main__":
    imgs = get_all_images()
    update_files(imgs)
