import os
import re
import requests
import random
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import quote  # <--- ÂÖ≥ÈîÆÂºïÂÖ•ÔºöÁî®‰∫éÁªôÈìæÊé•ÊâìÂåÖ

# --- 1. ÈÖçÁΩÆÂå∫Âüü ---
QUERIES = ["Â∞èË±ÜÊ≥• cat", "Â∞èË±ÜÊ≥• funny bean", "Â∞èË±ÜÊ≥• Êº´Áîª", "Â∞èË±ÜÊ≥• Ë°®ÊÉÖÂåÖ", "Â∞èË±ÜÊ≥• Âä®Áîª"]

# ÈªëÂêçÂçïÔºöËøáÊª§ÊéâÂÆπÊòìÂùèÁöÑÂõæÊ∫ê
ENGINE_DOMAINS = ["bing.com", "sogou.com", "so.com", "qhimg.com", "qhimgs.com", "baidu.com"]
DOMAIN_BLACKLIST = ["weibo.com", "sinaimg.cn", "zhimg.com", "csdnimg.cn", "127.net"]

# --- 2. Â∑•ÂÖ∑ÂáΩÊï∞ ---

def clean_url(url):
    """Ê∏ÖÊ¥ó URL ‰∏≠ÁöÑËΩ¨‰πâÂ≠óÁ¨¶"""
    if not url: return ""
    # ‰øÆÂ§ç JSON ËΩ¨‰πâ
    url = url.replace(r'\/', '/')
    # ‰øÆÂ§ç unicode ÁºñÁ†Å
    try:
        url = url.encode('utf-8').decode('unicode_escape')
    except:
        pass
    # ‰øÆÂ§ç HTML ÂÆû‰Ωì
    url = url.replace('&amp;', '&')
    return url

def wrap_proxy(url):
    """„Äê‰øÆÂ§çÊ†∏ÂøÉ„ÄëÂØπ URL ËøõË°åÁºñÁ†ÅÔºåÈò≤Ê≠¢ÂèÇÊï∞‰∏¢Â§±ÂØºËá¥ Not Found"""
    clean = clean_url(url)
    # quote Â∞ÜÈìæÊé•‰∏≠ÁöÑ & ? Á≠âÁ¨¶Âè∑ËΩ¨‰πâÔºåÁ°Æ‰øù‰ª£ÁêÜÊúçÂä°Âô®ËÉΩËØªÊáÇÂÆåÊï¥ÈìæÊé•
    encoded_url = quote(clean, safe='')
    # output=jpg: Âº∫Âà∂ËΩ¨Êç¢‰∏∫ jpg Ê†ºÂºèÔºåÂÖºÂÆπÊÄßÊúÄÂ•Ω
    return f"https://wsrv.nl/?url={encoded_url}&w=300&h=300&fit=cover&bg=white&output=jpg"

def is_valid(url, seen_urls, session_images):
    """ËøáÊª§Âô®"""
    if not url: return False
    url_lower = url.lower()
    
    if not url.startswith("http"): return False
    # ÂøÖÈ°ªÊòØÂ∏∏ËßÅÂõæÁâáÊ†ºÂºèÔºåÈÅøÂºÄÂ•áÊÄ™ÁöÑÂä®ÊÄÅËÑöÊú¨
    if not any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.gif', 'webp']): 
        # Êúâ‰∫õÂõæÂ∫äÈìæÊé•‰∏çÂ∏¶ÂêéÁºÄÔºåÂ¶ÇÊûúÊòØ bing/ÊêúÁãó ÊêúÂá∫Êù•ÁöÑÈÄöÂ∏∏Ê≤°ÈóÆÈ¢òÔºåÊîæÂÆΩ‰∏ÄÁÇπ
        if 'http' not in url_lower[4:]: # ÁÆÄÂçïÊ£ÄÊü•ÊòØ‰∏çÊòØÊ≠£Â∏∏ÁöÑ url
            pass
        else:
            return False

    if any(engine in url_lower for engine in ENGINE_DOMAINS): return False
    if any(bad in url_lower for bad in DOMAIN_BLACKLIST): return False
    if url in seen_urls or url in session_images: return False
    if any(x in url_lower for x in ["logo", "icon", "avatar", "sign", "symbol", "loading"]): return False
    
    return True

# --- 3. ÊäìÂèñÈÄªËæë ---

def fetch_from_bing(query, seen_urls, session_images, needed):
    print(f"üîç [Bing] Êêú: {query}")
    images = []
    first = random.randint(1, 10)
    url = f"https://www.bing.com/images/search?q={query}&first={first}&safeSearch=Moderate"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        links = re.findall(r'"murl":"(.*?)"', resp.text)
        for link in links:
            link = clean_url(link)
            if is_valid(link, seen_urls, session_images + images):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

def fetch_from_sogou(query, seen_urls, session_images, needed):
    print(f"üîç [Sogou] Êêú: {query}")
    images = []
    url = f"https://pic.sogou.com/pics?query={query}"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        links = re.findall(r'"thumbUrl":"(http[^"]+)"', resp.text)
        if not links:
            links = re.findall(r'https?://[^"\'\s]+\.(?:jpg|jpeg|png)', resp.text)
        for link in links:
            link = clean_url(link)
            if is_valid(link, seen_urls, session_images + images):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

def fetch_from_360(query, seen_urls, session_images, needed):
    print(f"üîç [360] Êêú: {query}")
    images = []
    url = f"https://image.so.com/i?q={query}"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        links = re.findall(r'"img":"(http[^"]+)"', resp.text)
        if not links:
            links = re.findall(r'https?://[^"\'\s]+\.(?:jpg|jpeg|png)', resp.text)
        for link in links:
            link = clean_url(link)
            if is_valid(link, seen_urls, session_images + images):
                images.append(link)
            if len(images) >= needed: break
    except: pass
    return images

# --- 4. ‰∏ªÊµÅÁ®ã ---

def get_all_images():
    seen = get_seen_urls()
    final_images = []
    fetchers = [fetch_from_bing, fetch_from_sogou, fetch_from_360]
    random.shuffle(fetchers)
    
    for fetcher in fetchers:
        needed = 12 - len(final_images)
        if needed <= 0: break
        query = random.choice(QUERIES)
        new_batch = fetcher(query, seen, final_images, needed)
        final_images.extend(new_batch)
    
    # Ë°•Ë¥ßÊú∫Âà∂
    if len(final_images) < 12:
        print("üí° Ë°•Ë¥ßÊ®°Âºè...")
        for q in QUERIES:
            if len(final_images) >= 12: break
            final_images.extend(fetch_f
