import os
import re
import requests
import random
from bs4 import BeautifulSoup
from datetime import datetime

# æ›´åŠ ç²¾å‡†çš„æœç´¢è¯ï¼Œå¢åŠ  cat å…³é”®è¯é˜²æ­¢æœåˆ°è±†å­
QUERIES = ["å°è±†æ³¥ cat", "å°è±†æ³¥ æ¼«ç”»", "funny bean cat", "å°è±†æ³¥ è¡¨æƒ…åŒ…", "å°è±†æ³¥ wallpaper"]
# åŸŸåé»‘åå•ï¼šæ’é™¤æœç´¢å¼•æ“è‡ªå®¶çš„ Logo å’Œå·²çŸ¥çš„åæº
BLACKLIST = ["bing.com/th", "bing.com/sa", "sogou.com", "so.com", "baidu.com", "weibo.com", "sinaimg.cn"]

def get_seen_urls():
    seen = set()
    if os.path.exists("history.md"):
        with open("history.md", "r", encoding="utf-8") as f:
            # æå–æ‰€æœ‰ http é“¾æ¥ï¼Œå¿½ç•¥ä»£ç†å‰ç¼€
            urls = re.findall(r'url=(http[^"\'&\s]+)', f.read())
            for u in urls:
                seen.add(u)
    return seen

def wrap_proxy(url):
    """é˜²ç›—é“¾+è£å‰ª+å¼ºåˆ¶ç™½åº•"""
    return f"https://wsrv.nl/?url={url}&w=300&h=300&fit=cover&bg=white"

def fetch_images(query, seen_urls, session_images, needed):
    print(f"ğŸ” æ­£åœ¨æœå¯»: {query}...")
    found = []
    # ä½¿ç”¨ Bing æœç´¢
    url = f"https://www.bing.com/images/search?q={query}&safeSearch=Moderate"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
    
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        # ç”¨æ­£åˆ™æŠ“å– murl (åŸå§‹å›¾ç‰‡åœ°å€)
        links = re.findall(r'"murl":"(.*?)"', resp.text)
        
        for link in links:
            # è¿‡æ»¤é»‘åå•ã€é‡å¤é¡¹ã€ä»¥åŠ Bing è‡ªå·±çš„ UI å›¾æ ‡
            if not link.startswith("http"): continue
            if any(b in link for b in BLACKLIST): continue
            if link in seen_urls or link in session_images or link in found: continue
            
            found.append(link)
            if len(found) >= needed: break
    except Exception as e:
        print(f"âš ï¸ æŠ“å–å‡ºé”™: {e}")
        
    return found

def get_all_images():
    seen = get_seen_urls()
    final_images = []
    
    # éšæœºæ‰“ä¹±å…³é”®è¯ï¼Œå¢åŠ æ–°é²œåº¦
    random.shuffle(QUERIES)
    
    for q in QUERIES:
        needed = 12 - len(final_images)
        if needed <= 0: break
        
        batch = fetch_images(q, seen, final_images, needed)
        final_images.extend(batch)
    
    print(f"ğŸ¯ æœ¬æ¬¡ä»»åŠ¡å…±æŠ“å–åˆ° {len(final_images)} å¼ æ–°å›¾")
    return [wrap_proxy(img) for img in final_images]

def update_files(urls):
    if not urls: 
        print("âŒ æ²¡æœåˆ°å›¾ï¼Œä¸æ›´æ–°ã€‚")
        return
        
    with open("README.md", "r", encoding="utf-8") as f:
        content = f.read()
    
    # æ„å»º 3x4 æˆ– 4x3 çš„ç²¾ç¾ç½‘æ ¼
    img_html = '<div align="center">\n'
    for url in urls:
        img_html += f'  <img src="{url}" width="160" height="160" alt="å°è±†æ³¥" style="margin:4px; border-radius:12px; object-fit:cover; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">'
    img_html += '\n  <p><i>ğŸ± æ¯æ—¥è‡ªåŠ¨æœé›†é«˜æ¸…å°è±†æ³¥ï¼Œæ’ç‰ˆå·²ä¼˜åŒ–</i></p>\n</div>'

    pattern = r"<!-- START_SECTION:xiaodouni -->.*?<!-- END_SECTION:xiaodouni -->"
    replacement = f"<!-- START_SECTION:xiaodouni -->\n{img_html}\n<!-- END_SECTION:xiaodouni -->"
    
    if "<!-- START_SECTION:xiaodouni -->" in content:
        new_readme = re.sub(pattern, replacement, content, flags=re.DOTALL)
        with open("README.md", "w", encoding="utf-8") as f:
            f.write(new_readme)

    # æ›´æ–° history.md
    today = datetime.now().strftime("%Y-%m-%d")
    if not os.path.exists("history.md"):
        with open("history.md", "w", encoding="utf-8") as f:
            f.write("# ğŸ“š å°è±†æ³¥å†å²æ”¶è—é¦†\n\n---\n")
            
    with open("history.md", "a", encoding="utf-8") as f:
        f.write(f"\n### ğŸ“… {today}\n<div align='left'>\n")
        for url in urls:
            f.write(f'  <img src="{url}" width="100" height="100" style="margin:2px; border-radius:6px; object-fit:cover;">\n')
        f.write("</div>\n\n---\n")

if __name__ == "__main__":
    imgs = get_all_images()
    update_files(imgs)
