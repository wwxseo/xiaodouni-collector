import os
import re
import requests
import random
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import quote

# --- 1. é…ç½®åŒºåŸŸ ---
QUERIES = ["å°è±†æ³¥ cat", "å°è±†æ³¥ funny bean", "å°è±†æ³¥ æ¼«ç”»", "å°è±†æ³¥ è¡¨æƒ…åŒ…", "å°è±†æ³¥ åŠ¨ç”»"]

# ä¸¥æ ¼é»‘åå•
ENGINE_DOMAINS = ["bing.com", "sogou.com", "so.com", "qhimg.com", "qhimgs.com", "baidu.com"]
DOMAIN_BLACKLIST = ["weibo.com", "sinaimg.cn", "zhimg.com", "csdnimg.cn", "127.net"]

# --- 2. å·¥å…·å‡½æ•° ---

def clean_url(url):
    """æ¸…æ´— URL"""
    if not url: return ""
    url = url.replace(r'\/', '/')
    try:
        url = url.encode('utf-8').decode('unicode_escape')
    except:
        pass
    url = url.replace('&amp;', '&')
    return url

def wrap_proxy(url):
    """ç”Ÿæˆä»£ç†é“¾æ¥"""
    clean = clean_url(url)
    encoded_url = quote(clean, safe='')
    # ä½¿ç”¨ images.weserv.nl (wsrv.nl çš„å…¨ç§°åŸŸåï¼Œæœ‰æ—¶æ›´ç¨³å®š)
    # output=jpg ç»Ÿä¸€æ ¼å¼ï¼Œw=300&h=300&fit=cover ç»Ÿä¸€æ’ç‰ˆ
    return f"https://images.weserv.nl/?url={encoded_url}&w=300&h=300&fit=cover&bg=white&output=jpg"

def check_image_availability(proxy_url):
    """ã€æ ¸å¿ƒä¿®å¤ã€‘è´¨æ£€å‘˜ï¼šäº²è‡ªéªŒè¯å›¾ç‰‡èƒ½ä¸èƒ½æ‰“å¼€"""
    try:
        # è®¾ç½® 3 ç§’è¶…æ—¶ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(proxy_url, headers=headers, timeout=3)
        if resp.status_code == 200:
            return True
        else:
            print(f"âš ï¸ å›¾ç‰‡æ— æ•ˆ (çŠ¶æ€ç  {resp.status_code}): {proxy_url[:50]}...")
            return False
    except Exception:
        print(f"âš ï¸ å›¾ç‰‡è¿æ¥è¶…æ—¶/é”™è¯¯: {proxy_url[:50]}...")
        return False

def is_valid_basic(url, seen_urls, session_images):
    """åŸºç¡€è¿‡æ»¤å™¨ï¼ˆä¸è”ç½‘ï¼‰"""
    if not url: return False
    url_lower = url.lower()
    if not url.startswith("http"): return False
    
    if any(engine in url_lower for engine in ENGINE_DOMAINS): return False
    if any(bad in url_lower for bad in DOMAIN_BLACKLIST): return False
    if url in seen_urls or url in session_images: return False
    if any(x in url_lower for x in ["logo", "icon", "avatar", "sign", "symbol", "loading", "gif"]): return False
    
    return True

def get_seen_urls():
    seen = set()
    if os.path.exists("history.md"):
        try:
            with open("history.md", "r", encoding="utf-8") as f:
                content = f.read()
                # æå– encoded çš„ url å‚æ•°ï¼Œé¿å…è§£ç é”™è¯¯
                matches = re.findall(r'url=([^&"\s]+)', content)
                for m in matches:
                    # ç®€å•è®°å½•ç‰¹å¾å³å¯ï¼Œä¸éœ€è¦å®Œç¾è§£ç 
                    seen.add(m)
        except Exception:
            pass
    return seen

# --- 3. æŠ“å–é€»è¾‘ ---

def fetch_images(engine_name, url_template, regex_pattern, query, seen_urls, session_images, needed):
    print(f"ğŸ” [{engine_name}] æœ: {query}")
    images = []
    
    # éšæœºåç§»
    search_url = url_template.format(query=query, random_first=random.randint(1, 5))
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    
    try:
        resp = requests.get(search_url, headers=headers, timeout=10)
        links = re.findall(regex_pattern, resp.text)
        
        for link in links:
            link = clean_url(link)
            
            # 1. åŸºç¡€æ£€æŸ¥
            if not is_valid_basic(link, seen_urls, session_images + images):
                continue
            
            # 2. ç”Ÿæˆä»£ç†é“¾æ¥
            proxy_link = wrap_proxy(link)
            
            # 3. ã€è´¨æ£€ã€‘è”ç½‘éªŒè¯ï¼åªæœ‰èƒ½æ‰“å¼€çš„æ‰æ”¶å½•
            if check_image_availability(proxy_link):
                print(f"âœ… æœ‰æ•ˆ: {link[:30]}...")
                images.append(link) # å­˜åŸå§‹é“¾æ¥é¿å…é‡å¤
            
            if len(images) >= needed: break
            
    except Exception as e:
        print(f"âŒ {engine_name} é”™è¯¯: {e}")
        pass
    
    return images

# --- 4. ä¸»æµç¨‹ ---

def get_all_images():
    seen = get_seen_urls()
    final_raw_links = [] # å­˜åŸå§‹é“¾æ¥ç”¨äºå»é‡
    
    # å®šä¹‰å¼•æ“
    engines = [
        ("Bing", "https://www.bing.com/images/search?q={query}&first={random_first}&safeSearch=Moderate", r'"murl":"(.*?)"'),
        ("Sogou", "https://pic.sogou.com/pics?query={query}", r'"thumbUrl":"(http[^"]+)"'),
        ("360", "https://image.so.com/i?q={query}", r'"img":"(http[^"]+)"')
    ]
    random.shuffle(engines)
    
    for name, url_tmpl, pattern in engines:
        needed = 12 - len(final_raw_links)
        if needed <= 0: break
        
        query = random.choice(QUERIES)
        # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥æŠŠéªŒè¯é€šè¿‡çš„å›¾åŠ è¿›æ¥
        new_batch = fetch_images(name, url_tmpl, pattern, query, seen, final_raw_links, needed)
        final_raw_links.extend(new_batch)
    
    # è¡¥è´§
    if len(final_raw_links) < 12:
        print("ğŸ’¡ è¡¥è´§æ¨¡å¼...")
        for q in QUERIES:
            if len(final_raw_links) >= 12: break
            # é»˜è®¤ç”¨ Bing è¡¥è´§
            new_batch = fetch_images("Bing", engines[0][1], engines[0][2], q, seen, final_raw_links, 12 - len(final_raw_links))
            final_raw_links.extend(new_batch)

    print(f"ğŸ¯ æœ¬æ¬¡æœ€ç»ˆæ•è· {len(final_raw_links)} å¼ æœ‰æ•ˆå›¾ç‰‡")
    # æœ€åç»Ÿä¸€è½¬ä¸ºä»£ç†é“¾æ¥
    return [wrap_proxy(url) for url in final_raw_links]

def update_files(urls):
    if not urls:
        print("âŒ æ— æœ‰æ•ˆå›¾ç‰‡ï¼Œè·³è¿‡æ›´æ–°ã€‚")
        return

    img_html = '<div align="center">\n'
    for url in urls:
        img_html += f'  <img src="{url}" width="160" height="160" alt="å°è±†æ³¥" style="margin:4px; border-radius:12px; object-fit:cover; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">'
    img_html += '\n  <p><i>ğŸ± æ¯æ—¥éšæœºä¸‰æºæœç½—ï¼Œåªé€‰é«˜æ¸…çŒ«çŒ«å¤´</i></p>\n</div>'

    if os.path.exists("README.md"):
        with open("README.md", "r", encoding="utf-8") as f:
            content = f.read()
        
        marker_start = "<!-- START_SECTION:xiaodouni -->"
        marker_end = "<!-- END_SECTION:xiaodouni -->"
        
        if marker_start in content and marker_end in content:
            s = content.find(marker_start) + len(marker_start)
            e = content.find(marker_end)
            new_content = content[:s] + "\n" + img_html + "\n" + content[e:]
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(new_content)
        else:
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(f"# å°è±†æ³¥æ”¶é›†å™¨\n\n{marker_start}\n{img_html}\n{marker_end}")
        print("âœ… README æ›´æ–°æˆåŠŸ")

    today = datetime.now().strftime("%Y-%m-%d")
    mode = "a" if os.path.exists("history.md") else "w"
    with open("history.md", mode, encoding="utf-8") as f:
        if mode == "w": f.write("# ğŸ“š å°è±†æ³¥å†å²æ”¶è—é¦†\n\n---\n")
        f.write(f"\n### ğŸ“… {today}\n<div align='left'>\n")
        for url in urls:
            f.write(f'  <img src="{url}" width="100" height="100" style="margin:2px; border-radius:6px; object-fit:cover;">\n')
        f.write("</div>\n\n---\n")
    print("âœ… History å½’æ¡£æˆåŠŸ")

if __name__ == "__main__":
    imgs = get_all_images()
    update_files(imgs)
